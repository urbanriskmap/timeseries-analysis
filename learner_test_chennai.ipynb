{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_colwidth = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 22:30:24,196 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:30:24,199 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:30:24,200 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:30:24,200 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:30:24,255 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:30:24,256 - DEBUG - AwsLabeler constructed\n",
      "2019-08-22 22:30:24,256 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:30:24,257 - DEBUG - AwsLabeler constructed\n"
     ]
    }
   ],
   "source": [
    "from loaders.cognicity_loader import CognicityLoader\n",
    "from learners.perceptron_learner import PerceptronLearner\n",
    "from learners.svm_learner import SvmLearner\n",
    "\n",
    "# import chennai_config\n",
    "# config = chennai_config.config\n",
    "\n",
    "import jakarta_config\n",
    "config = jakarta_config.config\n",
    "\n",
    "# import chennai_only_pics_config\n",
    "# config = chennai_only_pics_config.config\n",
    "\n",
    "RERUN = True\n",
    "\n",
    "from image_recognition.goog_recog import GoogleLabeler\n",
    "#goog_learner = PerceptronLearner(config, CognicityLoader, GoogleLabeler)\n",
    "goog_learner = SvmLearner(config, CognicityLoader, GoogleLabeler)\n",
    "\n",
    "from image_recognition.aws_recog import AwsLabeler\n",
    "#aws_learner = PerceptronLearner(config, CognicityLoader, AwsLabeler)\n",
    "aws_learner = SvmLearner(config, CognicityLoader, AwsLabeler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "validation_set = set(random.sample(config[\"flood_pkeys\"], 5))\n",
    "validation_set = validation_set.union(set(random.sample(config[\"no_flood_pkeys\"], 5)))\n",
    "len(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th, th0 = goog_learner.run_learner(\"goog_separator.p\", rerun=True, validation_keys=validation_set, params={\"T\":3000, \"print\":True})\n",
    "goog_learner.t_data_w_pkey[0,:]\n",
    "val = goog_learner.val_data_w_pkey[0,:]\n",
    "goog_learner.val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goog_learner.val_sd\n",
    "goog_learner.t_sd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th, th0 = aws_learner.run_learner(\"aws_separator.p\", rerun=RERUN, validation_keys=validation_set, params={\"T\":3000, \"print\":True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.shape\n",
    "aws_learner.t_sd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = aws_learner.labeler\n",
    "len(labs.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_labeler = aws_learner.labeler\n",
    "a = aws_labeler.config[\"flood_pkeys\"]\n",
    "b = aws_labeler.config[\"no_flood_pkeys\"]\n",
    "c = a.union(b)\n",
    "max(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp.bow_labeler import BowLabeler\n",
    "from learners.svm_learner import SvmLearner\n",
    "#bow_learner = PerceptronLearner(config, CognicityLoader, BowLabeler)\n",
    "bow_learner = SvmLearner(config, CognicityLoader, BowLabeler)\n",
    "th, th0 = bow_learner.run_learner(\"bow_separator.p\", rerun=RERUN, validation_keys=validation_set, params={\"T\":1000, \"print\":True})\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "data = bow_learner.t_data_w_pkey[1:,:].T\n",
    "labels = bow_learner.t_labels[0,:]\n",
    "\n",
    "s = bow_learner.cross_validate_model(data, labels,k=5)\n",
    "\n",
    "scores = cross_val_score(bow_learner.clf, data, labels, cv=10)\n",
    "\n",
    "scores.mean(), scores.std()*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s\n",
    "\n",
    "# chennai svm: \n",
    "# not random shuffle:\n",
    "# (0.761950146627566, 0.020836769409597528)\n",
    "# rand shuffle: \n",
    "# (0.725, 0.075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flood_depth.flood_labeler import IdentityLabeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 22:30:29,133 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:30:29,133 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:30:29,134 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:30:29,134 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:30:29,308 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:30:29,309 - DEBUG - AwsLabeler constructed\n",
      "2019-08-22 22:30:29,310 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:30:29,310 - DEBUG - AwsLabeler constructed\n",
      "2019-08-22 22:30:29,311 - DEBUG - logging from: default_jakarta_data/aws_labels_default.p\n",
      "2019-08-22 22:30:45,383 - INFO - Num Correct 19 Out of 29\n",
      "2019-08-22 22:30:45,384 - INFO - Val score: 0.6551724137931034\n",
      "2019-08-22 22:30:53,558 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:30:53,559 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:31:27,600 - INFO - Num Correct 21 Out of 29\n",
      "2019-08-22 22:31:27,601 - INFO - Val score: 0.7241379310344828\n",
      "2019-08-22 22:31:52,596 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:31:52,597 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:31:52,598 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:31:52,599 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:32:00,844 - INFO - Num Correct 20 Out of 29\n",
      "2019-08-22 22:32:00,845 - INFO - Val score: 0.6896551724137931\n",
      "2019-08-22 22:32:04,437 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:32:04,438 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:32:04,438 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:32:04,439 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:32:08,211 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:32:08,212 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:32:08,212 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:32:08,212 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:32:12,022 - DEBUG - logging from: default_jakarta_data/aws.p\n",
      "2019-08-22 22:32:12,025 - DEBUG - logging from: default_jakarta_data/goog.p\n",
      "2019-08-22 22:32:12,028 - DEBUG - logging from: default_jakarta_data/bow.p\n",
      "2019-08-22 22:32:12,035 - DEBUG - logging from: default_jakarta_data/fh.p\n",
      "2019-08-22 22:32:12,065 - INFO - training size (4, 2199)\n",
      "2019-08-22 22:32:12,067 - INFO - training matrix [[-1.5599041  -0.99975529 -1.00028157 ... -0.99977239  0.4099311\n",
      "  -0.14556993]\n",
      " [-1.26332127 -1.17706643 -0.89094562 ... -1.62482177  0.83522377\n",
      "   0.76968526]\n",
      " [-0.9999211  -1.00023194 -0.99976878 ... -0.2231107   0.12055919\n",
      "  -0.17848865]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "2019-08-22 22:32:12,084 - INFO - validation size (4, 30)\n",
      "/home/abrahamq/timeseries-analysis/simple_nn.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.output(y_pred)\n",
      "/home/abrahamq/timeseries-analysis/simple_nn.py:62: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: tensor(0.9030, grad_fn=<NllLossBackward>)\n",
      "Epoch: 500\n",
      "Loss: tensor(0.6511, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1000\n",
      "Loss: tensor(0.6190, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1500\n",
      "Loss: tensor(0.5938, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2000\n",
      "Loss: tensor(0.5715, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2500\n",
      "Loss: tensor(0.5418, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3000\n",
      "Loss: tensor(0.5220, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3500\n",
      "Loss: tensor(0.5052, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4000\n",
      "Loss: tensor(0.4907, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4500\n",
      "Loss: tensor(0.4784, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5000\n",
      "Loss: tensor(0.4679, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5500\n",
      "Loss: tensor(0.4591, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6000\n",
      "Loss: tensor(0.4517, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6500\n",
      "Loss: tensor(0.4454, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7000\n",
      "Loss: tensor(0.4402, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7500\n",
      "Loss: tensor(0.4358, grad_fn=<NllLossBackward>)\n",
      "Epoch: 8000\n",
      "Loss: tensor(0.4320, grad_fn=<NllLossBackward>)\n",
      "Epoch: 8500\n",
      "Loss: tensor(0.4288, grad_fn=<NllLossBackward>)\n",
      "Epoch: 9000\n",
      "Loss: tensor(0.4259, grad_fn=<NllLossBackward>)\n",
      "Epoch: 9500\n",
      "Loss: tensor(0.4230, grad_fn=<NllLossBackward>)\n",
      "Epoch: 10000\n",
      "Loss: tensor(0.3623, grad_fn=<NllLossBackward>)\n",
      "Epoch: 10500\n",
      "Loss: tensor(0.2795, grad_fn=<NllLossBackward>)\n",
      "Epoch: 11000\n",
      "Loss: tensor(0.2419, grad_fn=<NllLossBackward>)\n",
      "Epoch: 11500\n",
      "Loss: tensor(0.2229, grad_fn=<NllLossBackward>)\n",
      "Epoch: 12000\n",
      "Loss: tensor(0.2122, grad_fn=<NllLossBackward>)\n",
      "Epoch: 12500\n",
      "Loss: tensor(0.2057, grad_fn=<NllLossBackward>)\n",
      "Epoch: 13000\n",
      "Loss: tensor(0.2015, grad_fn=<NllLossBackward>)\n",
      "Epoch: 13500\n",
      "Loss: tensor(0.1986, grad_fn=<NllLossBackward>)\n",
      "Epoch: 14000\n",
      "Loss: tensor(0.1965, grad_fn=<NllLossBackward>)\n",
      "Epoch: 14500\n",
      "Loss: tensor(0.1950, grad_fn=<NllLossBackward>)\n",
      "Epoch: 15000\n",
      "Loss: tensor(0.1937, grad_fn=<NllLossBackward>)\n",
      "Epoch: 15500\n",
      "Loss: tensor(0.1928, grad_fn=<NllLossBackward>)\n",
      "Epoch: 16000\n",
      "Loss: tensor(0.1920, grad_fn=<NllLossBackward>)\n",
      "Epoch: 16500\n",
      "Loss: tensor(0.1914, grad_fn=<NllLossBackward>)\n",
      "Epoch: 17000\n",
      "Loss: tensor(0.1909, grad_fn=<NllLossBackward>)\n",
      "Epoch: 17500\n",
      "Loss: tensor(0.1905, grad_fn=<NllLossBackward>)\n",
      "Epoch: 18000\n",
      "Loss: tensor(0.1902, grad_fn=<NllLossBackward>)\n",
      "Epoch: 18500\n",
      "Loss: tensor(0.1899, grad_fn=<NllLossBackward>)\n",
      "Epoch: 19000\n",
      "Loss: tensor(0.1896, grad_fn=<NllLossBackward>)\n",
      "Epoch: 19500\n",
      "Loss: tensor(0.1894, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 22:32:39,764 - DEBUG - dumping model to: default_jakarta_data/chennai_en.p\n",
      "2019-08-22 22:32:39,828 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:32:39,829 - DEBUG - AwsLabeler constructed\n",
      "2019-08-22 22:32:39,830 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:32:39,830 - DEBUG - AwsLabeler constructed\n",
      "2019-08-22 22:32:39,831 - DEBUG - logging from: default_jakarta_data/aws_labels_default.p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score:  0.7333333333333333\n",
      "0.7333333333333333\n",
      "COMPARE WITH SVM: \n",
      "Num Correct 22 Out of 30\n",
      "Val score: 0.7333333333333333\n",
      "____________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 22:32:55,979 - INFO - Num Correct 14 Out of 29\n",
      "2019-08-22 22:32:55,980 - INFO - Val score: 0.4827586206896552\n",
      "2019-08-22 22:33:03,945 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:33:03,945 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:33:38,482 - INFO - Num Correct 17 Out of 28\n",
      "2019-08-22 22:33:38,483 - INFO - Val score: 0.6071428571428571\n",
      "2019-08-22 22:34:03,981 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:34:03,981 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:34:03,982 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:34:03,983 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:34:12,366 - INFO - Num Correct 19 Out of 29\n",
      "2019-08-22 22:34:12,367 - INFO - Val score: 0.6551724137931034\n",
      "2019-08-22 22:34:16,110 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:34:16,111 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:34:16,111 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:34:16,112 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:34:19,958 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:34:19,958 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:34:19,959 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:34:19,959 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:34:23,912 - DEBUG - logging from: default_jakarta_data/aws.p\n",
      "2019-08-22 22:34:23,913 - DEBUG - logging from: default_jakarta_data/goog.p\n",
      "2019-08-22 22:34:23,914 - DEBUG - logging from: default_jakarta_data/bow.p\n",
      "2019-08-22 22:34:23,916 - DEBUG - logging from: default_jakarta_data/fh.p\n",
      "2019-08-22 22:34:23,937 - INFO - training size (4, 2199)\n",
      "2019-08-22 22:34:23,938 - INFO - training matrix [[-1.51203323 -1.0000022  -0.99995778 ... -0.61405831 -0.99982555\n",
      "  -0.05976728]\n",
      " [-1.28376643 -1.1711741  -0.83971341 ... -0.94089009 -1.62505075\n",
      "   0.77183776]\n",
      " [-1.00017021 -1.00030254 -0.99981862 ... -0.9999812  -0.22707249\n",
      "  -0.13374913]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "2019-08-22 22:34:23,956 - INFO - validation size (4, 30)\n",
      "/home/abrahamq/timeseries-analysis/simple_nn.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.output(y_pred)\n",
      "/home/abrahamq/timeseries-analysis/simple_nn.py:62: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: tensor(0.9037, grad_fn=<NllLossBackward>)\n",
      "Epoch: 500\n",
      "Loss: tensor(0.6766, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1000\n",
      "Loss: tensor(0.6479, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1500\n",
      "Loss: tensor(0.6147, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2000\n",
      "Loss: tensor(0.5749, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2500\n",
      "Loss: tensor(0.5349, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3000\n",
      "Loss: tensor(0.4955, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3500\n",
      "Loss: tensor(0.4580, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4000\n",
      "Loss: tensor(0.4236, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4500\n",
      "Loss: tensor(0.3928, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5000\n",
      "Loss: tensor(0.3653, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5500\n",
      "Loss: tensor(0.3395, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6000\n",
      "Loss: tensor(0.3158, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6500\n",
      "Loss: tensor(0.2949, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7000\n",
      "Loss: tensor(0.2771, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7500\n",
      "Loss: tensor(0.2622, grad_fn=<NllLossBackward>)\n",
      "Epoch: 8000\n",
      "Loss: tensor(0.2498, grad_fn=<NllLossBackward>)\n",
      "Epoch: 8500\n",
      "Loss: tensor(0.2382, grad_fn=<NllLossBackward>)\n",
      "Epoch: 9000\n",
      "Loss: tensor(0.2288, grad_fn=<NllLossBackward>)\n",
      "Epoch: 9500\n",
      "Loss: tensor(0.2226, grad_fn=<NllLossBackward>)\n",
      "Epoch: 10000\n",
      "Loss: tensor(0.2176, grad_fn=<NllLossBackward>)\n",
      "Epoch: 10500\n",
      "Loss: tensor(0.2135, grad_fn=<NllLossBackward>)\n",
      "Epoch: 11000\n",
      "Loss: tensor(0.2101, grad_fn=<NllLossBackward>)\n",
      "Epoch: 11500\n",
      "Loss: tensor(0.2074, grad_fn=<NllLossBackward>)\n",
      "Epoch: 12000\n",
      "Loss: tensor(0.2051, grad_fn=<NllLossBackward>)\n",
      "Epoch: 12500\n",
      "Loss: tensor(0.2032, grad_fn=<NllLossBackward>)\n",
      "Epoch: 13000\n",
      "Loss: tensor(0.2015, grad_fn=<NllLossBackward>)\n",
      "Epoch: 13500\n",
      "Loss: tensor(0.2002, grad_fn=<NllLossBackward>)\n",
      "Epoch: 14000\n",
      "Loss: tensor(0.1990, grad_fn=<NllLossBackward>)\n",
      "Epoch: 14500\n",
      "Loss: tensor(0.1980, grad_fn=<NllLossBackward>)\n",
      "Epoch: 15000\n",
      "Loss: tensor(0.1971, grad_fn=<NllLossBackward>)\n",
      "Epoch: 15500\n",
      "Loss: tensor(0.1964, grad_fn=<NllLossBackward>)\n",
      "Epoch: 16000\n",
      "Loss: tensor(0.1957, grad_fn=<NllLossBackward>)\n",
      "Epoch: 16500\n",
      "Loss: tensor(0.1951, grad_fn=<NllLossBackward>)\n",
      "Epoch: 17000\n",
      "Loss: tensor(0.1946, grad_fn=<NllLossBackward>)\n",
      "Epoch: 17500\n",
      "Loss: tensor(0.1941, grad_fn=<NllLossBackward>)\n",
      "Epoch: 18000\n",
      "Loss: tensor(0.1937, grad_fn=<NllLossBackward>)\n",
      "Epoch: 18500\n",
      "Loss: tensor(0.1933, grad_fn=<NllLossBackward>)\n",
      "Epoch: 19000\n",
      "Loss: tensor(0.1929, grad_fn=<NllLossBackward>)\n",
      "Epoch: 19500\n",
      "Loss: tensor(0.1925, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 22:34:50,942 - DEBUG - dumping model to: default_jakarta_data/chennai_en.p\n",
      "2019-08-22 22:34:51,003 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:34:51,004 - DEBUG - AwsLabeler constructed\n",
      "2019-08-22 22:34:51,005 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:34:51,006 - DEBUG - AwsLabeler constructed\n",
      "2019-08-22 22:34:51,006 - DEBUG - logging from: default_jakarta_data/aws_labels_default.p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score:  0.5333333333333333\n",
      "0.5333333333333333\n",
      "COMPARE WITH SVM: \n",
      "Num Correct 19 Out of 30\n",
      "Val score: 0.6333333333333333\n",
      "____________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 22:35:07,578 - INFO - Num Correct 18 Out of 29\n",
      "2019-08-22 22:35:07,578 - INFO - Val score: 0.6206896551724138\n",
      "2019-08-22 22:35:15,325 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:35:15,326 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:35:49,807 - INFO - Num Correct 22 Out of 29\n",
      "2019-08-22 22:35:49,808 - INFO - Val score: 0.7586206896551724\n",
      "2019-08-22 22:36:15,542 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:36:15,543 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:36:15,543 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:36:15,544 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:36:24,326 - INFO - Num Correct 21 Out of 29\n",
      "2019-08-22 22:36:24,327 - INFO - Val score: 0.7241379310344828\n",
      "2019-08-22 22:36:27,943 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:36:27,944 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:36:27,944 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:36:27,945 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:36:31,841 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:36:31,842 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:36:31,843 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:36:31,843 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:36:35,772 - DEBUG - logging from: default_jakarta_data/aws.p\n",
      "2019-08-22 22:36:35,774 - DEBUG - logging from: default_jakarta_data/goog.p\n",
      "2019-08-22 22:36:35,775 - DEBUG - logging from: default_jakarta_data/bow.p\n",
      "2019-08-22 22:36:35,776 - DEBUG - logging from: default_jakarta_data/fh.p\n",
      "2019-08-22 22:36:35,796 - INFO - training size (4, 2199)\n",
      "2019-08-22 22:36:35,796 - INFO - training matrix [[-1.5311734  -1.00018937 -0.99974645 ... -1.0003176   0.40921588\n",
      "  -0.10736911]\n",
      " [-1.25384425 -1.19945877 -0.8266305  ... -1.63554322  0.8618044\n",
      "   0.80478821]\n",
      " [-1.00018173 -0.99985113 -1.00001401 ... -0.25116671  0.11730511\n",
      "  -0.19545929]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "2019-08-22 22:36:35,809 - INFO - validation size (4, 30)\n",
      "/home/abrahamq/timeseries-analysis/simple_nn.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.output(y_pred)\n",
      "/home/abrahamq/timeseries-analysis/simple_nn.py:62: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: tensor(0.7474, grad_fn=<NllLossBackward>)\n",
      "Epoch: 500\n",
      "Loss: tensor(0.4901, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1000\n",
      "Loss: tensor(0.4022, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1500\n",
      "Loss: tensor(0.3409, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2000\n",
      "Loss: tensor(0.2974, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2500\n",
      "Loss: tensor(0.2671, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3000\n",
      "Loss: tensor(0.2459, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3500\n",
      "Loss: tensor(0.2310, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4000\n",
      "Loss: tensor(0.2202, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4500\n",
      "Loss: tensor(0.2124, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5000\n",
      "Loss: tensor(0.2065, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5500\n",
      "Loss: tensor(0.2021, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6000\n",
      "Loss: tensor(0.1987, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6500\n",
      "Loss: tensor(0.1960, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7000\n",
      "Loss: tensor(0.1939, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7500\n",
      "Loss: tensor(0.1922, grad_fn=<NllLossBackward>)\n",
      "Epoch: 8000\n",
      "Loss: tensor(0.1909, grad_fn=<NllLossBackward>)\n",
      "Epoch: 8500\n",
      "Loss: tensor(0.1899, grad_fn=<NllLossBackward>)\n",
      "Epoch: 9000\n",
      "Loss: tensor(0.1890, grad_fn=<NllLossBackward>)\n",
      "Epoch: 9500\n",
      "Loss: tensor(0.1883, grad_fn=<NllLossBackward>)\n",
      "Epoch: 10000\n",
      "Loss: tensor(0.1877, grad_fn=<NllLossBackward>)\n",
      "Epoch: 10500\n",
      "Loss: tensor(0.1872, grad_fn=<NllLossBackward>)\n",
      "Epoch: 11000\n",
      "Loss: tensor(0.1868, grad_fn=<NllLossBackward>)\n",
      "Epoch: 11500\n",
      "Loss: tensor(0.1865, grad_fn=<NllLossBackward>)\n",
      "Epoch: 12000\n",
      "Loss: tensor(0.1862, grad_fn=<NllLossBackward>)\n",
      "Epoch: 12500\n",
      "Loss: tensor(0.1859, grad_fn=<NllLossBackward>)\n",
      "Epoch: 13000\n",
      "Loss: tensor(0.1857, grad_fn=<NllLossBackward>)\n",
      "Epoch: 13500\n",
      "Loss: tensor(0.1856, grad_fn=<NllLossBackward>)\n",
      "Epoch: 14000\n",
      "Loss: tensor(0.1854, grad_fn=<NllLossBackward>)\n",
      "Epoch: 14500\n",
      "Loss: tensor(0.1853, grad_fn=<NllLossBackward>)\n",
      "Epoch: 15000\n",
      "Loss: tensor(0.1851, grad_fn=<NllLossBackward>)\n",
      "Epoch: 15500\n",
      "Loss: tensor(0.1850, grad_fn=<NllLossBackward>)\n",
      "Epoch: 16000\n",
      "Loss: tensor(0.1849, grad_fn=<NllLossBackward>)\n",
      "Epoch: 16500\n",
      "Loss: tensor(0.1848, grad_fn=<NllLossBackward>)\n",
      "Epoch: 17000\n",
      "Loss: tensor(0.1848, grad_fn=<NllLossBackward>)\n",
      "Epoch: 17500\n",
      "Loss: tensor(0.1847, grad_fn=<NllLossBackward>)\n",
      "Epoch: 18000\n",
      "Loss: tensor(0.1846, grad_fn=<NllLossBackward>)\n",
      "Epoch: 18500\n",
      "Loss: tensor(0.1845, grad_fn=<NllLossBackward>)\n",
      "Epoch: 19000\n",
      "Loss: tensor(0.1845, grad_fn=<NllLossBackward>)\n",
      "Epoch: 19500\n",
      "Loss: tensor(0.1844, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 22:37:04,929 - DEBUG - dumping model to: default_jakarta_data/chennai_en.p\n",
      "2019-08-22 22:37:04,990 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:37:04,991 - DEBUG - AwsLabeler constructed\n",
      "2019-08-22 22:37:04,992 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:37:04,992 - DEBUG - AwsLabeler constructed\n",
      "2019-08-22 22:37:04,993 - DEBUG - logging from: default_jakarta_data/aws_labels_default.p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score:  0.6666666666666666\n",
      "0.6666666666666666\n",
      "COMPARE WITH SVM: \n",
      "Num Correct 21 Out of 30\n",
      "Val score: 0.7\n",
      "____________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 22:37:21,907 - INFO - Num Correct 22 Out of 29\n",
      "2019-08-22 22:37:21,908 - INFO - Val score: 0.7586206896551724\n",
      "2019-08-22 22:37:29,582 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:37:29,582 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:38:04,655 - INFO - Num Correct 21 Out of 30\n",
      "2019-08-22 22:38:04,655 - INFO - Val score: 0.7\n",
      "2019-08-22 22:38:29,957 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:38:29,959 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:38:29,959 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:38:29,960 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:38:38,416 - INFO - Num Correct 23 Out of 29\n",
      "2019-08-22 22:38:38,417 - INFO - Val score: 0.7931034482758621\n",
      "2019-08-22 22:38:42,040 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:38:42,041 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:38:42,042 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:38:42,043 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:38:46,018 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:38:46,019 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:38:46,019 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:38:46,020 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:38:50,024 - DEBUG - logging from: default_jakarta_data/aws.p\n",
      "2019-08-22 22:38:50,026 - DEBUG - logging from: default_jakarta_data/goog.p\n",
      "2019-08-22 22:38:50,027 - DEBUG - logging from: default_jakarta_data/bow.p\n",
      "2019-08-22 22:38:50,028 - DEBUG - logging from: default_jakarta_data/fh.p\n",
      "2019-08-22 22:38:50,045 - INFO - training size (4, 2199)\n",
      "2019-08-22 22:38:50,046 - INFO - training matrix [[-1.57868969 -1.00026878 -0.99990431 ... -0.99990947  0.42501552\n",
      "  -0.11254606]\n",
      " [-1.28553242 -1.1251288  -0.86144998 ... -1.88848048  0.89093743\n",
      "   0.84444883]\n",
      " [-1.00154638 -1.00001526 -0.99962818 ... -0.22445576  0.12690278\n",
      "  -0.18197824]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "2019-08-22 22:38:50,060 - INFO - validation size (4, 30)\n",
      "/home/abrahamq/timeseries-analysis/simple_nn.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.output(y_pred)\n",
      "/home/abrahamq/timeseries-analysis/simple_nn.py:62: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: tensor(0.7210, grad_fn=<NllLossBackward>)\n",
      "Epoch: 500\n",
      "Loss: tensor(0.6452, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1000\n",
      "Loss: tensor(0.6066, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1500\n",
      "Loss: tensor(0.5856, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2000\n",
      "Loss: tensor(0.5671, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2500\n",
      "Loss: tensor(0.5502, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3000\n",
      "Loss: tensor(0.5350, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3500\n",
      "Loss: tensor(0.5209, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4000\n",
      "Loss: tensor(0.4921, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4500\n",
      "Loss: tensor(0.4022, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5000\n",
      "Loss: tensor(0.3391, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5500\n",
      "Loss: tensor(0.2994, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6000\n",
      "Loss: tensor(0.2728, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6500\n",
      "Loss: tensor(0.2538, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7000\n",
      "Loss: tensor(0.2398, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7500\n",
      "Loss: tensor(0.2292, grad_fn=<NllLossBackward>)\n",
      "Epoch: 8000\n",
      "Loss: tensor(0.2211, grad_fn=<NllLossBackward>)\n",
      "Epoch: 8500\n",
      "Loss: tensor(0.2147, grad_fn=<NllLossBackward>)\n",
      "Epoch: 9000\n",
      "Loss: tensor(0.2097, grad_fn=<NllLossBackward>)\n",
      "Epoch: 9500\n",
      "Loss: tensor(0.2057, grad_fn=<NllLossBackward>)\n",
      "Epoch: 10000\n",
      "Loss: tensor(0.2025, grad_fn=<NllLossBackward>)\n",
      "Epoch: 10500\n",
      "Loss: tensor(0.1999, grad_fn=<NllLossBackward>)\n",
      "Epoch: 11000\n",
      "Loss: tensor(0.1978, grad_fn=<NllLossBackward>)\n",
      "Epoch: 11500\n",
      "Loss: tensor(0.1961, grad_fn=<NllLossBackward>)\n",
      "Epoch: 12000\n",
      "Loss: tensor(0.1946, grad_fn=<NllLossBackward>)\n",
      "Epoch: 12500\n",
      "Loss: tensor(0.1934, grad_fn=<NllLossBackward>)\n",
      "Epoch: 13000\n",
      "Loss: tensor(0.1925, grad_fn=<NllLossBackward>)\n",
      "Epoch: 13500\n",
      "Loss: tensor(0.1917, grad_fn=<NllLossBackward>)\n",
      "Epoch: 14000\n",
      "Loss: tensor(0.1910, grad_fn=<NllLossBackward>)\n",
      "Epoch: 14500\n",
      "Loss: tensor(0.1904, grad_fn=<NllLossBackward>)\n",
      "Epoch: 15000\n",
      "Loss: tensor(0.1899, grad_fn=<NllLossBackward>)\n",
      "Epoch: 15500\n",
      "Loss: tensor(0.1895, grad_fn=<NllLossBackward>)\n",
      "Epoch: 16000\n",
      "Loss: tensor(0.1891, grad_fn=<NllLossBackward>)\n",
      "Epoch: 16500\n",
      "Loss: tensor(0.1888, grad_fn=<NllLossBackward>)\n",
      "Epoch: 17000\n",
      "Loss: tensor(0.1885, grad_fn=<NllLossBackward>)\n",
      "Epoch: 17500\n",
      "Loss: tensor(0.1883, grad_fn=<NllLossBackward>)\n",
      "Epoch: 18000\n",
      "Loss: tensor(0.1880, grad_fn=<NllLossBackward>)\n",
      "Epoch: 18500\n",
      "Loss: tensor(0.1878, grad_fn=<NllLossBackward>)\n",
      "Epoch: 19000\n",
      "Loss: tensor(0.1876, grad_fn=<NllLossBackward>)\n",
      "Epoch: 19500\n",
      "Loss: tensor(0.1875, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 22:39:17,629 - DEBUG - dumping model to: default_jakarta_data/chennai_en.p\n",
      "2019-08-22 22:39:17,691 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:39:17,692 - DEBUG - AwsLabeler constructed\n",
      "2019-08-22 22:39:17,693 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:39:17,693 - DEBUG - AwsLabeler constructed\n",
      "2019-08-22 22:39:17,694 - DEBUG - logging from: default_jakarta_data/aws_labels_default.p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score:  0.7333333333333333\n",
      "0.7333333333333333\n",
      "COMPARE WITH SVM: \n",
      "Num Correct 22 Out of 30\n",
      "Val score: 0.7333333333333333\n",
      "____________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 22:39:35,067 - INFO - Num Correct 20 Out of 28\n",
      "2019-08-22 22:39:35,068 - INFO - Val score: 0.7142857142857143\n",
      "2019-08-22 22:39:42,783 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:39:42,784 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:40:18,333 - INFO - Num Correct 20 Out of 29\n",
      "2019-08-22 22:40:18,335 - INFO - Val score: 0.6896551724137931\n",
      "2019-08-22 22:40:44,197 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:40:44,198 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:40:44,198 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:40:44,199 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:40:53,398 - INFO - Num Correct 19 Out of 28\n",
      "2019-08-22 22:40:53,399 - INFO - Val score: 0.6785714285714286\n",
      "2019-08-22 22:40:57,187 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:40:57,188 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:40:57,189 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:40:57,190 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:41:01,447 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:41:01,447 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:41:01,448 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:41:01,448 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:41:05,669 - DEBUG - logging from: default_jakarta_data/aws.p\n",
      "2019-08-22 22:41:05,670 - DEBUG - logging from: default_jakarta_data/goog.p\n",
      "2019-08-22 22:41:05,671 - DEBUG - logging from: default_jakarta_data/bow.p\n",
      "2019-08-22 22:41:05,673 - DEBUG - logging from: default_jakarta_data/fh.p\n",
      "2019-08-22 22:41:05,686 - INFO - training size (4, 2199)\n",
      "2019-08-22 22:41:05,687 - INFO - training matrix [[-1.51858873 -0.99986737 -1.0000812  ... -1.00006864  0.41828056\n",
      "  -0.10701011]\n",
      " [-1.30677291 -1.15047148 -0.85065908 ... -1.6599388   0.85256824\n",
      "   0.7531557 ]\n",
      " [-1.0003788  -0.9996292  -0.99979438 ... -0.21438723  0.11345341\n",
      "  -0.18832692]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "2019-08-22 22:41:05,702 - INFO - validation size (4, 30)\n",
      "/home/abrahamq/timeseries-analysis/simple_nn.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.output(y_pred)\n",
      "/home/abrahamq/timeseries-analysis/simple_nn.py:62: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: tensor(0.8356, grad_fn=<NllLossBackward>)\n",
      "Epoch: 500\n",
      "Loss: tensor(0.6592, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1000\n",
      "Loss: tensor(0.6408, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1500\n",
      "Loss: tensor(0.6249, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2000\n",
      "Loss: tensor(0.6084, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2500\n",
      "Loss: tensor(0.5900, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3000\n",
      "Loss: tensor(0.5689, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3500\n",
      "Loss: tensor(0.5313, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4000\n",
      "Loss: tensor(0.4666, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4500\n",
      "Loss: tensor(0.4091, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5000\n",
      "Loss: tensor(0.3615, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5500\n",
      "Loss: tensor(0.3237, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6000\n",
      "Loss: tensor(0.2810, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6500\n",
      "Loss: tensor(0.2603, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7000\n",
      "Loss: tensor(0.2450, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7500\n",
      "Loss: tensor(0.2333, grad_fn=<NllLossBackward>)\n",
      "Epoch: 8000\n",
      "Loss: tensor(0.2242, grad_fn=<NllLossBackward>)\n",
      "Epoch: 8500\n",
      "Loss: tensor(0.2171, grad_fn=<NllLossBackward>)\n",
      "Epoch: 9000\n",
      "Loss: tensor(0.2114, grad_fn=<NllLossBackward>)\n",
      "Epoch: 9500\n",
      "Loss: tensor(0.2068, grad_fn=<NllLossBackward>)\n",
      "Epoch: 10000\n",
      "Loss: tensor(0.2032, grad_fn=<NllLossBackward>)\n",
      "Epoch: 10500\n",
      "Loss: tensor(0.2003, grad_fn=<NllLossBackward>)\n",
      "Epoch: 11000\n",
      "Loss: tensor(0.1979, grad_fn=<NllLossBackward>)\n",
      "Epoch: 11500\n",
      "Loss: tensor(0.1959, grad_fn=<NllLossBackward>)\n",
      "Epoch: 12000\n",
      "Loss: tensor(0.1943, grad_fn=<NllLossBackward>)\n",
      "Epoch: 12500\n",
      "Loss: tensor(0.1930, grad_fn=<NllLossBackward>)\n",
      "Epoch: 13000\n",
      "Loss: tensor(0.1919, grad_fn=<NllLossBackward>)\n",
      "Epoch: 13500\n",
      "Loss: tensor(0.1911, grad_fn=<NllLossBackward>)\n",
      "Epoch: 14000\n",
      "Loss: tensor(0.1903, grad_fn=<NllLossBackward>)\n",
      "Epoch: 14500\n",
      "Loss: tensor(0.1897, grad_fn=<NllLossBackward>)\n",
      "Epoch: 15000\n",
      "Loss: tensor(0.1893, grad_fn=<NllLossBackward>)\n",
      "Epoch: 15500\n",
      "Loss: tensor(0.1888, grad_fn=<NllLossBackward>)\n",
      "Epoch: 16000\n",
      "Loss: tensor(0.1885, grad_fn=<NllLossBackward>)\n",
      "Epoch: 16500\n",
      "Loss: tensor(0.1882, grad_fn=<NllLossBackward>)\n",
      "Epoch: 17000\n",
      "Loss: tensor(0.1880, grad_fn=<NllLossBackward>)\n",
      "Epoch: 17500\n",
      "Loss: tensor(0.1878, grad_fn=<NllLossBackward>)\n",
      "Epoch: 18000\n",
      "Loss: tensor(0.1876, grad_fn=<NllLossBackward>)\n",
      "Epoch: 18500\n",
      "Loss: tensor(0.1874, grad_fn=<NllLossBackward>)\n",
      "Epoch: 19000\n",
      "Loss: tensor(0.1873, grad_fn=<NllLossBackward>)\n",
      "Epoch: 19500\n",
      "Loss: tensor(0.1871, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 22:41:35,201 - DEBUG - dumping model to: default_jakarta_data/chennai_en.p\n",
      "2019-08-22 22:41:35,262 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:41:35,263 - DEBUG - AwsLabeler constructed\n",
      "2019-08-22 22:41:35,264 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:41:35,264 - DEBUG - AwsLabeler constructed\n",
      "2019-08-22 22:41:35,265 - DEBUG - logging from: default_jakarta_data/aws_labels_default.p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score:  0.6333333333333333\n",
      "0.6333333333333333\n",
      "COMPARE WITH SVM: \n",
      "Num Correct 20 Out of 30\n",
      "Val score: 0.6666666666666666\n",
      "____________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 22:41:53,757 - INFO - Num Correct 17 Out of 28\n",
      "2019-08-22 22:41:53,770 - INFO - Val score: 0.6071428571428571\n",
      "2019-08-22 22:42:01,987 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:42:01,990 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:42:41,692 - INFO - Num Correct 18 Out of 29\n",
      "2019-08-22 22:42:41,693 - INFO - Val score: 0.6206896551724138\n",
      "2019-08-22 22:43:08,892 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:43:08,897 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:43:08,898 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:43:08,899 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:43:18,512 - INFO - Num Correct 18 Out of 28\n",
      "2019-08-22 22:43:18,513 - INFO - Val score: 0.6428571428571429\n",
      "2019-08-22 22:43:22,389 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:43:22,390 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:43:22,390 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:43:22,391 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:43:27,025 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:43:27,027 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:43:27,028 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:43:27,028 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:43:31,656 - DEBUG - logging from: default_jakarta_data/aws.p\n",
      "2019-08-22 22:43:31,659 - DEBUG - logging from: default_jakarta_data/goog.p\n",
      "2019-08-22 22:43:31,662 - DEBUG - logging from: default_jakarta_data/bow.p\n",
      "2019-08-22 22:43:31,670 - DEBUG - logging from: default_jakarta_data/fh.p\n",
      "2019-08-22 22:43:31,689 - INFO - training size (4, 2199)\n",
      "2019-08-22 22:43:31,690 - INFO - training matrix [[-1.56732278 -1.00025886 -1.00012666 ... -1.00009986  0.43246719\n",
      "  -0.14154441]\n",
      " [-1.29217921 -1.17749186 -0.83866183 ... -1.67223211  0.88792711\n",
      "   0.6401481 ]\n",
      " [-1.00018824 -0.99961074 -1.00047032 ... -0.23189611  0.12260871\n",
      "  -0.170152  ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "2019-08-22 22:43:31,707 - INFO - validation size (4, 30)\n",
      "/home/abrahamq/timeseries-analysis/simple_nn.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.output(y_pred)\n",
      "/home/abrahamq/timeseries-analysis/simple_nn.py:62: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: tensor(0.6823, grad_fn=<NllLossBackward>)\n",
      "Epoch: 500\n",
      "Loss: tensor(0.6260, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1000\n",
      "Loss: tensor(0.5924, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1500\n",
      "Loss: tensor(0.5331, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2000\n",
      "Loss: tensor(0.4590, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2500\n",
      "Loss: tensor(0.3964, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3000\n",
      "Loss: tensor(0.3459, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3500\n",
      "Loss: tensor(0.3075, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4000\n",
      "Loss: tensor(0.2794, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4500\n",
      "Loss: tensor(0.2590, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5000\n",
      "Loss: tensor(0.2442, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5500\n",
      "Loss: tensor(0.2333, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6000\n",
      "Loss: tensor(0.2253, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6500\n",
      "Loss: tensor(0.2192, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7000\n",
      "Loss: tensor(0.2146, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7500\n",
      "Loss: tensor(0.2110, grad_fn=<NllLossBackward>)\n",
      "Epoch: 8000\n",
      "Loss: tensor(0.2081, grad_fn=<NllLossBackward>)\n",
      "Epoch: 8500\n",
      "Loss: tensor(0.2059, grad_fn=<NllLossBackward>)\n",
      "Epoch: 9000\n",
      "Loss: tensor(0.2040, grad_fn=<NllLossBackward>)\n",
      "Epoch: 9500\n",
      "Loss: tensor(0.2025, grad_fn=<NllLossBackward>)\n",
      "Epoch: 10000\n",
      "Loss: tensor(0.2012, grad_fn=<NllLossBackward>)\n",
      "Epoch: 10500\n",
      "Loss: tensor(0.2001, grad_fn=<NllLossBackward>)\n",
      "Epoch: 11000\n",
      "Loss: tensor(0.1993, grad_fn=<NllLossBackward>)\n",
      "Epoch: 11500\n",
      "Loss: tensor(0.1985, grad_fn=<NllLossBackward>)\n",
      "Epoch: 12000\n",
      "Loss: tensor(0.1979, grad_fn=<NllLossBackward>)\n",
      "Epoch: 12500\n",
      "Loss: tensor(0.1973, grad_fn=<NllLossBackward>)\n",
      "Epoch: 13000\n",
      "Loss: tensor(0.1968, grad_fn=<NllLossBackward>)\n",
      "Epoch: 13500\n",
      "Loss: tensor(0.1964, grad_fn=<NllLossBackward>)\n",
      "Epoch: 14000\n",
      "Loss: tensor(0.1960, grad_fn=<NllLossBackward>)\n",
      "Epoch: 14500\n",
      "Loss: tensor(0.1957, grad_fn=<NllLossBackward>)\n",
      "Epoch: 15000\n",
      "Loss: tensor(0.1954, grad_fn=<NllLossBackward>)\n",
      "Epoch: 15500\n",
      "Loss: tensor(0.1951, grad_fn=<NllLossBackward>)\n",
      "Epoch: 16000\n",
      "Loss: tensor(0.1949, grad_fn=<NllLossBackward>)\n",
      "Epoch: 16500\n",
      "Loss: tensor(0.1946, grad_fn=<NllLossBackward>)\n",
      "Epoch: 17000\n",
      "Loss: tensor(0.1944, grad_fn=<NllLossBackward>)\n",
      "Epoch: 17500\n",
      "Loss: tensor(0.1942, grad_fn=<NllLossBackward>)\n",
      "Epoch: 18000\n",
      "Loss: tensor(0.1940, grad_fn=<NllLossBackward>)\n",
      "Epoch: 18500\n",
      "Loss: tensor(0.1938, grad_fn=<NllLossBackward>)\n",
      "Epoch: 19000\n",
      "Loss: tensor(0.1937, grad_fn=<NllLossBackward>)\n",
      "Epoch: 19500\n",
      "Loss: tensor(0.1935, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 22:44:15,491 - DEBUG - dumping model to: default_jakarta_data/chennai_en.p\n",
      "2019-08-22 22:44:15,555 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:44:15,557 - DEBUG - AwsLabeler constructed\n",
      "2019-08-22 22:44:15,557 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:44:15,557 - DEBUG - AwsLabeler constructed\n",
      "2019-08-22 22:44:15,558 - DEBUG - logging from: default_jakarta_data/aws_labels_default.p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score:  0.6333333333333333\n",
      "0.6333333333333333\n",
      "COMPARE WITH SVM: \n",
      "Num Correct 20 Out of 30\n",
      "Val score: 0.6666666666666666\n",
      "____________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 22:44:34,493 - INFO - Num Correct 21 Out of 28\n",
      "2019-08-22 22:44:34,494 - INFO - Val score: 0.75\n",
      "2019-08-22 22:44:42,301 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:44:42,302 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:45:17,190 - INFO - Num Correct 19 Out of 26\n",
      "2019-08-22 22:45:17,191 - INFO - Val score: 0.7307692307692307\n",
      "2019-08-22 22:45:42,460 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:45:42,461 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:45:42,461 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:45:42,462 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:45:51,398 - INFO - Num Correct 19 Out of 28\n",
      "2019-08-22 22:45:51,398 - INFO - Val score: 0.6785714285714286\n",
      "2019-08-22 22:45:55,087 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:45:55,087 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:45:55,088 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:45:55,089 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:45:59,386 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:45:59,387 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:45:59,387 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:45:59,388 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:46:03,728 - DEBUG - logging from: default_jakarta_data/aws.p\n",
      "2019-08-22 22:46:03,731 - DEBUG - logging from: default_jakarta_data/goog.p\n",
      "2019-08-22 22:46:03,733 - DEBUG - logging from: default_jakarta_data/bow.p\n",
      "2019-08-22 22:46:03,740 - DEBUG - logging from: default_jakarta_data/fh.p\n",
      "2019-08-22 22:46:03,764 - INFO - training size (4, 2199)\n",
      "2019-08-22 22:46:03,766 - INFO - training matrix [[-1.53024442 -0.99992587 -0.99987639 ... -1.00021078  0.42747045\n",
      "  -0.13830062]\n",
      " [-1.23734015 -1.25564234 -0.91939083 ... -1.60339082  0.85810793\n",
      "   0.77885865]\n",
      " [-0.99999726 -0.99967955 -1.00004281 ... -0.21241263  0.13891262\n",
      "  -0.15899695]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "2019-08-22 22:46:03,780 - INFO - validation size (4, 30)\n",
      "/home/abrahamq/timeseries-analysis/simple_nn.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.output(y_pred)\n",
      "/home/abrahamq/timeseries-analysis/simple_nn.py:62: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: tensor(0.6700, grad_fn=<NllLossBackward>)\n",
      "Epoch: 500\n",
      "Loss: tensor(0.6128, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1000\n",
      "Loss: tensor(0.5782, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1500\n",
      "Loss: tensor(0.5528, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2000\n",
      "Loss: tensor(0.5340, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2500\n",
      "Loss: tensor(0.5201, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3000\n",
      "Loss: tensor(0.5096, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3500\n",
      "Loss: tensor(0.5014, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4000\n",
      "Loss: tensor(0.4947, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4500\n",
      "Loss: tensor(0.4891, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5000\n",
      "Loss: tensor(0.4842, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5500\n",
      "Loss: tensor(0.4798, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6000\n",
      "Loss: tensor(0.4759, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6500\n",
      "Loss: tensor(0.4723, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7000\n",
      "Loss: tensor(0.4691, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7500\n",
      "Loss: tensor(0.4660, grad_fn=<NllLossBackward>)\n",
      "Epoch: 8000\n",
      "Loss: tensor(0.4632, grad_fn=<NllLossBackward>)\n",
      "Epoch: 8500\n",
      "Loss: tensor(0.4604, grad_fn=<NllLossBackward>)\n",
      "Epoch: 9000\n",
      "Loss: tensor(0.4578, grad_fn=<NllLossBackward>)\n",
      "Epoch: 9500\n",
      "Loss: tensor(0.4553, grad_fn=<NllLossBackward>)\n",
      "Epoch: 10000\n",
      "Loss: tensor(0.4528, grad_fn=<NllLossBackward>)\n",
      "Epoch: 10500\n",
      "Loss: tensor(0.4505, grad_fn=<NllLossBackward>)\n",
      "Epoch: 11000\n",
      "Loss: tensor(0.4484, grad_fn=<NllLossBackward>)\n",
      "Epoch: 11500\n",
      "Loss: tensor(0.4463, grad_fn=<NllLossBackward>)\n",
      "Epoch: 12000\n",
      "Loss: tensor(0.4443, grad_fn=<NllLossBackward>)\n",
      "Epoch: 12500\n",
      "Loss: tensor(0.4423, grad_fn=<NllLossBackward>)\n",
      "Epoch: 13000\n",
      "Loss: tensor(0.4403, grad_fn=<NllLossBackward>)\n",
      "Epoch: 13500\n",
      "Loss: tensor(0.4383, grad_fn=<NllLossBackward>)\n",
      "Epoch: 14000\n",
      "Loss: tensor(0.4355, grad_fn=<NllLossBackward>)\n",
      "Epoch: 14500\n",
      "Loss: tensor(0.4094, grad_fn=<NllLossBackward>)\n",
      "Epoch: 15000\n",
      "Loss: tensor(0.3477, grad_fn=<NllLossBackward>)\n",
      "Epoch: 15500\n",
      "Loss: tensor(0.3003, grad_fn=<NllLossBackward>)\n",
      "Epoch: 16000\n",
      "Loss: tensor(0.2681, grad_fn=<NllLossBackward>)\n",
      "Epoch: 16500\n",
      "Loss: tensor(0.2473, grad_fn=<NllLossBackward>)\n",
      "Epoch: 17000\n",
      "Loss: tensor(0.2331, grad_fn=<NllLossBackward>)\n",
      "Epoch: 17500\n",
      "Loss: tensor(0.2231, grad_fn=<NllLossBackward>)\n",
      "Epoch: 18000\n",
      "Loss: tensor(0.2159, grad_fn=<NllLossBackward>)\n",
      "Epoch: 18500\n",
      "Loss: tensor(0.2106, grad_fn=<NllLossBackward>)\n",
      "Epoch: 19000\n",
      "Loss: tensor(0.2066, grad_fn=<NllLossBackward>)\n",
      "Epoch: 19500\n",
      "Loss: tensor(0.2034, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 22:46:46,210 - DEBUG - dumping model to: default_jakarta_data/chennai_en.p\n",
      "2019-08-22 22:46:46,277 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:46:46,278 - DEBUG - AwsLabeler constructed\n",
      "2019-08-22 22:46:46,279 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:46:46,280 - DEBUG - AwsLabeler constructed\n",
      "2019-08-22 22:46:46,281 - DEBUG - logging from: default_jakarta_data/aws_labels_default.p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score:  0.7\n",
      "0.7\n",
      "COMPARE WITH SVM: \n",
      "Num Correct 21 Out of 30\n",
      "Val score: 0.7\n",
      "____________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 22:47:07,971 - INFO - Num Correct 19 Out of 29\n",
      "2019-08-22 22:47:07,972 - INFO - Val score: 0.6551724137931034\n",
      "2019-08-22 22:47:16,667 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:47:16,668 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:47:56,193 - INFO - Num Correct 21 Out of 30\n",
      "2019-08-22 22:47:56,193 - INFO - Val score: 0.7\n",
      "2019-08-22 22:48:21,900 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:48:21,901 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:48:21,902 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:48:21,903 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:48:30,529 - INFO - Num Correct 14 Out of 29\n",
      "2019-08-22 22:48:30,530 - INFO - Val score: 0.4827586206896552\n",
      "2019-08-22 22:48:34,348 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:48:34,349 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:48:34,350 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:48:34,351 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:48:38,794 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:48:38,795 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:48:38,795 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:48:38,796 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:48:43,223 - DEBUG - logging from: default_jakarta_data/aws.p\n",
      "2019-08-22 22:48:43,227 - DEBUG - logging from: default_jakarta_data/goog.p\n",
      "2019-08-22 22:48:43,229 - DEBUG - logging from: default_jakarta_data/bow.p\n",
      "2019-08-22 22:48:43,235 - DEBUG - logging from: default_jakarta_data/fh.p\n",
      "2019-08-22 22:48:43,259 - INFO - training size (4, 2199)\n",
      "2019-08-22 22:48:43,260 - INFO - training matrix [[-1.54275557 -0.99986716 -0.99999001 ... -1.00003125  0.42321938\n",
      "  -0.10436835]\n",
      " [-1.26709454 -1.29294546 -0.91579111 ... -1.64484832  0.87862967\n",
      "   0.85389554]\n",
      " [-0.99999765 -0.99972569 -0.99986499 ... -0.26667361  0.15289916\n",
      "  -0.14946205]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "2019-08-22 22:48:43,272 - INFO - validation size (4, 30)\n",
      "/home/abrahamq/timeseries-analysis/simple_nn.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.output(y_pred)\n",
      "/home/abrahamq/timeseries-analysis/simple_nn.py:62: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: tensor(0.6653, grad_fn=<NllLossBackward>)\n",
      "Epoch: 500\n",
      "Loss: tensor(0.5490, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1000\n",
      "Loss: tensor(0.4625, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1500\n",
      "Loss: tensor(0.3971, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2000\n",
      "Loss: tensor(0.3480, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2500\n",
      "Loss: tensor(0.3110, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3000\n",
      "Loss: tensor(0.2831, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3500\n",
      "Loss: tensor(0.2620, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4000\n",
      "Loss: tensor(0.2459, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4500\n",
      "Loss: tensor(0.2335, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5000\n",
      "Loss: tensor(0.2240, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5500\n",
      "Loss: tensor(0.2167, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6000\n",
      "Loss: tensor(0.2109, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6500\n",
      "Loss: tensor(0.2064, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7000\n",
      "Loss: tensor(0.2028, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7500\n",
      "Loss: tensor(0.1999, grad_fn=<NllLossBackward>)\n",
      "Epoch: 8000\n",
      "Loss: tensor(0.1977, grad_fn=<NllLossBackward>)\n",
      "Epoch: 8500\n",
      "Loss: tensor(0.1958, grad_fn=<NllLossBackward>)\n",
      "Epoch: 9000\n",
      "Loss: tensor(0.1943, grad_fn=<NllLossBackward>)\n",
      "Epoch: 9500\n",
      "Loss: tensor(0.1931, grad_fn=<NllLossBackward>)\n",
      "Epoch: 10000\n",
      "Loss: tensor(0.1921, grad_fn=<NllLossBackward>)\n",
      "Epoch: 10500\n",
      "Loss: tensor(0.1913, grad_fn=<NllLossBackward>)\n",
      "Epoch: 11000\n",
      "Loss: tensor(0.1906, grad_fn=<NllLossBackward>)\n",
      "Epoch: 11500\n",
      "Loss: tensor(0.1900, grad_fn=<NllLossBackward>)\n",
      "Epoch: 12000\n",
      "Loss: tensor(0.1896, grad_fn=<NllLossBackward>)\n",
      "Epoch: 12500\n",
      "Loss: tensor(0.1892, grad_fn=<NllLossBackward>)\n",
      "Epoch: 13000\n",
      "Loss: tensor(0.1888, grad_fn=<NllLossBackward>)\n",
      "Epoch: 13500\n",
      "Loss: tensor(0.1885, grad_fn=<NllLossBackward>)\n",
      "Epoch: 14000\n",
      "Loss: tensor(0.1883, grad_fn=<NllLossBackward>)\n",
      "Epoch: 14500\n",
      "Loss: tensor(0.1881, grad_fn=<NllLossBackward>)\n",
      "Epoch: 15000\n",
      "Loss: tensor(0.1879, grad_fn=<NllLossBackward>)\n",
      "Epoch: 15500\n",
      "Loss: tensor(0.1878, grad_fn=<NllLossBackward>)\n",
      "Epoch: 16000\n",
      "Loss: tensor(0.1877, grad_fn=<NllLossBackward>)\n",
      "Epoch: 16500\n",
      "Loss: tensor(0.1875, grad_fn=<NllLossBackward>)\n",
      "Epoch: 17000\n",
      "Loss: tensor(0.1874, grad_fn=<NllLossBackward>)\n",
      "Epoch: 17500\n",
      "Loss: tensor(0.1874, grad_fn=<NllLossBackward>)\n",
      "Epoch: 18000\n",
      "Loss: tensor(0.1873, grad_fn=<NllLossBackward>)\n",
      "Epoch: 18500\n",
      "Loss: tensor(0.1872, grad_fn=<NllLossBackward>)\n",
      "Epoch: 19000\n",
      "Loss: tensor(0.1872, grad_fn=<NllLossBackward>)\n",
      "Epoch: 19500\n",
      "Loss: tensor(0.1871, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 22:49:24,857 - DEBUG - dumping model to: default_jakarta_data/chennai_en.p\n",
      "2019-08-22 22:49:24,922 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:49:24,923 - DEBUG - AwsLabeler constructed\n",
      "2019-08-22 22:49:24,924 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:49:24,924 - DEBUG - AwsLabeler constructed\n",
      "2019-08-22 22:49:24,925 - DEBUG - logging from: default_jakarta_data/aws_labels_default.p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score:  0.6\n",
      "0.6\n",
      "COMPARE WITH SVM: \n",
      "Num Correct 20 Out of 30\n",
      "Val score: 0.6666666666666666\n",
      "____________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 22:49:47,807 - INFO - Num Correct 18 Out of 26\n",
      "2019-08-22 22:49:47,808 - INFO - Val score: 0.6923076923076923\n",
      "2019-08-22 22:49:56,427 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:49:56,428 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:50:36,072 - INFO - Num Correct 25 Out of 30\n",
      "2019-08-22 22:50:36,073 - INFO - Val score: 0.8333333333333334\n",
      "2019-08-22 22:51:04,314 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:51:04,315 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:51:04,316 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:51:04,317 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:51:16,200 - INFO - Num Correct 14 Out of 26\n",
      "2019-08-22 22:51:16,201 - INFO - Val score: 0.5384615384615384\n",
      "2019-08-22 22:51:20,350 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:51:20,352 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:51:20,353 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:51:20,353 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:51:26,938 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:51:26,942 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:51:26,942 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:51:26,944 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:51:33,832 - DEBUG - logging from: default_jakarta_data/aws.p\n",
      "2019-08-22 22:51:33,834 - DEBUG - logging from: default_jakarta_data/goog.p\n",
      "2019-08-22 22:51:33,836 - DEBUG - logging from: default_jakarta_data/bow.p\n",
      "2019-08-22 22:51:33,839 - DEBUG - logging from: default_jakarta_data/fh.p\n",
      "2019-08-22 22:51:33,861 - INFO - training size (4, 2199)\n",
      "2019-08-22 22:51:33,862 - INFO - training matrix [[-1.58751307 -0.99992068 -1.00011163 ... -0.99968217  0.43217366\n",
      "  -0.10447946]\n",
      " [-1.27629695 -1.18030335 -0.87575357 ... -1.63146221  0.88923881\n",
      "   0.80231872]\n",
      " [-0.99999815 -1.00018095 -1.00031924 ... -0.23551079  0.13797448\n",
      "  -0.15583346]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "2019-08-22 22:51:33,880 - INFO - validation size (4, 30)\n",
      "/home/abrahamq/timeseries-analysis/simple_nn.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.output(y_pred)\n",
      "/home/abrahamq/timeseries-analysis/simple_nn.py:62: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: tensor(0.6931, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 22:51:34,171 - DEBUG - dumping model to: default_jakarta_data/chennai_en.p\n",
      "2019-08-22 22:51:34,259 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:51:34,260 - DEBUG - AwsLabeler constructed\n",
      "2019-08-22 22:51:34,261 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:51:34,263 - DEBUG - AwsLabeler constructed\n",
      "2019-08-22 22:51:34,263 - DEBUG - logging from: default_jakarta_data/aws_labels_default.p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch: 40\n",
      "Ensemble Score:  0.5\n",
      "0.5\n",
      "COMPARE WITH SVM: \n",
      "Num Correct 23 Out of 30\n",
      "Val score: 0.7666666666666667\n",
      "____________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 22:51:56,372 - INFO - Num Correct 19 Out of 30\n",
      "2019-08-22 22:51:56,373 - INFO - Val score: 0.6333333333333333\n",
      "2019-08-22 22:52:04,338 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:52:04,339 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:52:41,037 - INFO - Num Correct 23 Out of 30\n",
      "2019-08-22 22:52:41,039 - INFO - Val score: 0.7666666666666667\n",
      "2019-08-22 22:53:05,850 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:53:05,855 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:53:05,855 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:53:05,856 - DEBUG - GoogleLabeler constructed\n",
      "2019-08-22 22:53:14,921 - INFO - Num Correct 20 Out of 30\n",
      "2019-08-22 22:53:14,922 - INFO - Val score: 0.6666666666666666\n",
      "2019-08-22 22:53:18,391 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:53:18,392 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:53:18,393 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:53:18,394 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:53:22,540 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:53:22,541 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:53:22,541 - DEBUG - CognicityLoader constructed\n",
      "2019-08-22 22:53:22,542 - DEBUG - IdentityLabeler constructed\n",
      "2019-08-22 22:53:26,634 - DEBUG - logging from: default_jakarta_data/aws.p\n",
      "2019-08-22 22:53:26,637 - DEBUG - logging from: default_jakarta_data/goog.p\n",
      "2019-08-22 22:53:26,639 - DEBUG - logging from: default_jakarta_data/bow.p\n",
      "2019-08-22 22:53:26,646 - DEBUG - logging from: default_jakarta_data/fh.p\n",
      "2019-08-22 22:53:26,722 - INFO - training size (4, 2199)\n",
      "2019-08-22 22:53:26,723 - INFO - training matrix [[-1.54135463 -0.99973523 -0.99998491 ... -1.00004454  0.49508711\n",
      "  -0.15130634]\n",
      " [-1.26514782 -1.1324303  -0.87652661 ... -1.6534426   0.8567239\n",
      "   0.83665953]\n",
      " [-0.99971505 -1.00006212 -1.00031098 ... -0.20565598  0.13261744\n",
      "  -0.13075133]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "2019-08-22 22:53:26,734 - INFO - validation size (4, 30)\n",
      "/home/abrahamq/timeseries-analysis/simple_nn.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.output(y_pred)\n",
      "/home/abrahamq/timeseries-analysis/simple_nn.py:62: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: tensor(0.6361, grad_fn=<NllLossBackward>)\n",
      "Epoch: 500\n",
      "Loss: tensor(0.5998, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1000\n",
      "Loss: tensor(0.5697, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1500\n",
      "Loss: tensor(0.5181, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2000\n",
      "Loss: tensor(0.4550, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2500\n",
      "Loss: tensor(0.4043, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3000\n",
      "Loss: tensor(0.3633, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3500\n",
      "Loss: tensor(0.3301, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4000\n",
      "Loss: tensor(0.3036, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4500\n",
      "Loss: tensor(0.2829, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5000\n",
      "Loss: tensor(0.2669, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5500\n",
      "Loss: tensor(0.2545, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6000\n",
      "Loss: tensor(0.2451, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6500\n",
      "Loss: tensor(0.2378, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7000\n",
      "Loss: tensor(0.2322, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7500\n",
      "Loss: tensor(0.2278, grad_fn=<NllLossBackward>)\n",
      "Epoch: 8000\n",
      "Loss: tensor(0.2243, grad_fn=<NllLossBackward>)\n",
      "Epoch: 8500\n",
      "Loss: tensor(0.2216, grad_fn=<NllLossBackward>)\n",
      "Epoch: 9000\n",
      "Loss: tensor(0.2194, grad_fn=<NllLossBackward>)\n",
      "Epoch: 9500\n",
      "Loss: tensor(0.2176, grad_fn=<NllLossBackward>)\n",
      "Epoch: 10000\n",
      "Loss: tensor(0.2161, grad_fn=<NllLossBackward>)\n",
      "Epoch: 10500\n",
      "Loss: tensor(0.2149, grad_fn=<NllLossBackward>)\n",
      "Epoch: 11000\n",
      "Loss: tensor(0.2140, grad_fn=<NllLossBackward>)\n",
      "Epoch: 11500\n",
      "Loss: tensor(0.2131, grad_fn=<NllLossBackward>)\n",
      "Epoch: 12000\n",
      "Loss: tensor(0.2124, grad_fn=<NllLossBackward>)\n",
      "Epoch: 12500\n",
      "Loss: tensor(0.2118, grad_fn=<NllLossBackward>)\n",
      "Epoch: 13000\n",
      "Loss: tensor(0.2113, grad_fn=<NllLossBackward>)\n",
      "Epoch: 13500\n",
      "Loss: tensor(0.2109, grad_fn=<NllLossBackward>)\n",
      "Epoch: 14000\n",
      "Loss: tensor(0.2105, grad_fn=<NllLossBackward>)\n",
      "Epoch: 14500\n",
      "Loss: tensor(0.2102, grad_fn=<NllLossBackward>)\n",
      "Epoch: 15000\n",
      "Loss: tensor(0.2099, grad_fn=<NllLossBackward>)\n",
      "Epoch: 15500\n",
      "Loss: tensor(0.2097, grad_fn=<NllLossBackward>)\n",
      "Epoch: 16000\n",
      "Loss: tensor(0.2094, grad_fn=<NllLossBackward>)\n",
      "Epoch: 16500\n",
      "Loss: tensor(0.2092, grad_fn=<NllLossBackward>)\n",
      "Epoch: 17000\n",
      "Loss: tensor(0.2090, grad_fn=<NllLossBackward>)\n",
      "Epoch: 17500\n",
      "Loss: tensor(0.2089, grad_fn=<NllLossBackward>)\n",
      "Epoch: 18000\n",
      "Loss: tensor(0.2087, grad_fn=<NllLossBackward>)\n",
      "Epoch: 18500\n",
      "Loss: tensor(0.2085, grad_fn=<NllLossBackward>)\n",
      "Epoch: 19000\n",
      "Loss: tensor(0.2084, grad_fn=<NllLossBackward>)\n",
      "Epoch: 19500\n",
      "Loss: tensor(0.2083, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 22:53:53,479 - DEBUG - dumping model to: default_jakarta_data/chennai_en.p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score:  0.6666666666666666\n",
      "0.6666666666666666\n",
      "COMPARE WITH SVM: \n",
      "Num Correct 19 Out of 30\n",
      "Val score: 0.6333333333333333\n",
      "____________\n",
      "[0.7333333333333333, 0.5333333333333333, 0.6666666666666666, 0.7333333333333333, 0.6333333333333333, 0.6333333333333333, 0.7, 0.6, 0.5, 0.6666666666666666]\n"
     ]
    }
   ],
   "source": [
    "from learners.identity_learner import IdentityLearner\n",
    "from learners.identity_learner import IdentityLearner\n",
    "from flood_depth.flood_labeler import IdentityLabeler\n",
    "from image_recognition.goog_recog import GoogleLabeler\n",
    "from nlp.bow_labeler import BowLabeler\n",
    "\n",
    "identity_learner = IdentityLearner(config, CognicityLoader, IdentityLabeler)\n",
    "\n",
    "\n",
    "from learners.ensemble_learner import EnsembleLearner\n",
    "import random\n",
    "\n",
    "fin = []\n",
    "results= dict()\n",
    "for i in range(5, 15):\n",
    "    \n",
    "    validation_set = set(random.sample(config[\"flood_pkeys\"], 15))\n",
    "    validation_set = validation_set.union(set(random.sample(config[\"no_flood_pkeys\"], 15)))\n",
    "    \n",
    "    # from sklearn.model_selection import ShuffleSplit\n",
    "    # rs = ShuffleSplit(n_splits =5, test_size=.10)\n",
    "    # \n",
    "    # \n",
    "    aws_learner = SvmLearner(config, CognicityLoader, AwsLabeler)\n",
    "    th, th0 = aws_learner.run_learner(\"aws_separator.p\", rerun=RERUN, validation_keys=validation_set, params={\"T\":1000, \"print\":True})\n",
    "    \n",
    "    bow_learner = SvmLearner(config, CognicityLoader, BowLabeler)\n",
    "    th, th0 = bow_learner.run_learner(\"bow_separator.p\", rerun=RERUN, validation_keys=validation_set, params={\"T\":400, \"print\":True})\n",
    "    \n",
    "    goog_learner = SvmLearner(config, CognicityLoader, GoogleLabeler)\n",
    "    th, th0 = goog_learner.run_learner(\"goog_separator.p\", rerun=True, validation_keys=validation_set, params={\"T\":1000, \"print\":True})\n",
    "    \n",
    "    fh_learner = IdentityLearner(config, CognicityLoader, IdentityLabeler)\n",
    "    meh = fh_learner.run_learner(\"iden_separator.p\", rerun=RERUN, validation_keys=validation_set, params={\"T\":400, \"print\":True})\n",
    "    \n",
    "    learners = [aws_learner, goog_learner, bow_learner, fh_learner]\n",
    "    names = [\"aws.p\", \"goog.p\", \"bow.p\", \"fh.p\"]\n",
    "    \n",
    "    # learners = [aws_learner, goog_learner, bow_learner]\n",
    "    # names = [\"aws.p\", \"goog.p\", \"bow.p\"]\n",
    "    \n",
    "    # aws_learner = SvmLearner(config, CognicityLoader, AwsLabeler)\n",
    "    # th, th0 = aws_learner.run_learner(\"aws_separator.p\", rerun=RERUN, validation_keys=validation_set, params={\"T\":1000, \"print\":True})\n",
    "    # bow_learner = SvmLearner(config, CognicityLoader, BowLabeler)\n",
    "    # th, th0 = bow_learner.run_learner(\"bow_separator.p\", rerun=RERUN, validation_keys=validation_set, params={\"T\":400, \"print\":True})\n",
    "    # \n",
    "    # learners = [aws_learner, bow_learner]\n",
    "    # names = [\"aws.p\", \"bow.p\"]\n",
    "    \n",
    "    fh_learner = IdentityLearner(config, CognicityLoader, IdentityLabeler)\n",
    "    meh = fh_learner.run_learner(\"iden_separator.p\", rerun=RERUN, validation_keys=validation_set, params={\"T\":400, \"print\":True})\n",
    "    \n",
    "    \n",
    "    en_learner = EnsembleLearner(config, names, learners)\n",
    "    \n",
    "    \n",
    "    \n",
    "    nn_model = en_learner.run_learner(\"chennai_en.p\", rerun=RERUN, validation_keys=validation_set, params={\"hidden\":i, \"print\":True, \"T\":5000})\n",
    "    import math\n",
    "    probs = math.e**en_learner.res\n",
    "    probs\n",
    "    import numpy as np\n",
    "    p = probs.data.numpy()\n",
    "    predicted = np.argmax(p, axis=1)\n",
    "    val_labs = en_learner.val_labels\n",
    "    true = np.where(val_labs <0, 0, 1)\n",
    "    score = np.sum(predicted == true)/(true.shape[0])\n",
    "    fin.append(score)\n",
    "    print(\"Ensemble Score: \", score)\n",
    "    \n",
    "    import math\n",
    "    probs = math.e**en_learner.res\n",
    "    probs\n",
    "    \n",
    "    import numpy as np\n",
    "    p = probs.data.numpy()\n",
    "    predicted = np.argmax(p, axis=1)\n",
    "    val_labs = en_learner.val_labels\n",
    "    true = np.where(val_labs <0, 0, 1)\n",
    "    score = np.sum(predicted == true)/(true.shape[0])\n",
    "    results[i] = (score, validation_set, en_learner, val_labs)\n",
    "    print(score)\n",
    "    \n",
    "    print(\"COMPARE WITH SVM: \")\n",
    "    t_data = en_learner.train_matrix\n",
    "    t_labels = en_learner.t_labels\n",
    "    val_data = en_learner.val_matrix\n",
    "    val_labels  = en_learner.val_labels\n",
    "    \n",
    "    from sklearn import svm\n",
    "    clf = svm.SVC(gamma=\"scale\", kernel=\"rbf\", degree=3)\n",
    "            # sklearn expects rows to be data points, we've gone with columns\n",
    "    clf.fit(t_data.T, t_labels)\n",
    "    \n",
    "    pred = clf.predict(val_data.T)\n",
    "    \n",
    "    correct = np.sum(val_labels == pred)\n",
    "    \n",
    "    total = val_data.shape[1]\n",
    "    percent_correct = correct/total\n",
    "    print(\"Num Correct \" + str(correct) +\n",
    "                     \" Out of \" + str(total))\n",
    "    print(\"Val score: \" + str(percent_correct))\n",
    "    print(\"____________\")\n",
    "\n",
    "# mispredicted keys:\n",
    "\n",
    "print(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# make sure to shuffle the data!\n",
    "cv = ShuffleSplit(n_splits=k, test_size=.10)\n",
    "scores = cross_val_score(self.clf, data, labels, cv=cv)\n",
    "self.scores = scores\n",
    "return (scores.mean(), scores.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# print(en_learner.val_matrix)\n",
    "# print(en_learner.val_matrix[-1,:])\n",
    "# print(predicted)\n",
    "# np.sum(np.sum(en_learner.train_matrix[1:-1,:], axis=0) == 0)\n",
    "val_matrix = en_learner.val_matrix\n",
    "label_matrix = en_learner.val_labels\n",
    "\n",
    "\n",
    "into_sk = val_matrix.T\n",
    "print(into_sk.shape)\n",
    "into_torch = torch.from_numpy(into_sk).float()\n",
    "\n",
    "en_learner.predict(en_learner.val_matrix.T)\n",
    "\n",
    "# from simple_nn import Simple_nn\n",
    "# \n",
    "# nn = Simple_nn(into_sk.shape[1], 10)\n",
    "# nn.forward(into_torch)\n",
    "# \n",
    "# toy = EnsembleLearner(config, [3, 3], [3, 3])\n",
    "# \n",
    "# \n",
    "# print(into_sk.shape)\n",
    "# print(into_torch.size)\n",
    "# toy.fit(into_sk, label_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data = en_learner.train_matrix\n",
    "t_labels = en_learner.t_labels\n",
    "val_data = en_learner.val_matrix\n",
    "val_labels  = en_learner.val_labels\n",
    "\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma=\"scale\", kernel=\"poly\", degree=3)\n",
    "        # sklearn expects rows to be data points, we've gone with columns\n",
    "clf.fit(t_data.T, t_labels)\n",
    "\n",
    "pred = clf.predict(val_data.T)\n",
    "\n",
    "correct = np.sum(val_labels == pred)\n",
    "\n",
    "total = val_data.shape[1]\n",
    "percent_correct = correct/total\n",
    "print(\"Num Correct \" + str(correct) +\n",
    "                 \" Out of \" + str(total))\n",
    "print(\"Val score: \" + str(percent_correct))\n",
    "print(pred)\n",
    "print(val_labels)\n",
    "t_data[:,2:5]\n",
    "\n",
    "len(config[\"flood_pkeys\"])/len(config[\"all_pkeys\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pkeys = goog_learner.val_data_w_pkey[0, (predicted != true)[0,:]]\n",
    "\n",
    "import pandas as pd\n",
    "start_known_flood = \"'2017-11-01 00:00:35.630000-04:00'\" \n",
    "end_known_flood = \"'2019-11-07 00:00:35.630000-04:00'\"\n",
    "\n",
    "chennai_all_data = pd.read_sql_query('''\n",
    "    SELECT pkey, created_at, text, disaster_type, report_data, tags, image_url FROM riskmap.all_reports \n",
    "''', params={\"start_date\": start_known_flood, \"end_date\": end_known_flood}, con=config[\"database_engine\"], index_col=\"pkey\")\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "chennai_all_data.loc[pd.Index(wrong_pkeys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = list(range(1, 7))\n",
    "a.extend([7 for i in range(10)])\n",
    "a\n",
    "diff = a[-1] - a[0]\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.pop(0)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted\n",
    "\n",
    "validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "a = np.array([[1, 2, 4, 65, 75],\n",
    "         [113, 222, 4444, 65455, 7777]])\n",
    "\n",
    "b = np.array([[1, 2, 4, 65, 70, 75]])\n",
    "\n",
    "c = np.vstack((b, np.zeros((1, b.shape[1]))))\n",
    "\n",
    "b_df = pd.Index(b.T)\n",
    "\n",
    "c_df = pd.DataFrame(c.T, columns=[\"pkey\", \"data\"])\n",
    "\n",
    "a_df = pd.DataFrame(a.T, columns=[\"pkey\", \"data\"] )\n",
    "\n",
    "c_df[\"pkey\"] == a_df[\"pkey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "a = np.array([[1, 2, 4, 65, 75],\n",
    "         [113, 222, 4444, 65455, 7777]])\n",
    "\n",
    "labs = np.array([[1, 1, 1, -1, -1]])\n",
    "b = np.array([[1, 2, 4, 65, 70, 75]])\n",
    "\n",
    "c = np.vstack((b, np.zeros((1, b.shape[1]))))\n",
    "\n",
    "b_df = pd.Index(b.T)\n",
    "\n",
    "c_df = pd.DataFrame(c.T, columns=[\"pkey\", \"data\"]).set_index(\"pkey\", drop=False)\n",
    "\n",
    "a_df = pd.DataFrame(a.T, columns=[\"pkey\", \"data\"] )\n",
    "# a_df.rename_axis( \"pkey\")\n",
    "a_df.set_index(\"pkey\", inplace=True, drop=False)\n",
    "a_df\n",
    "\n",
    "# c_df.loc[a_df.index] = a_df[\"data\"]\n",
    "c_df.loc[a_df.index] = a_df.loc[a_df.index]\n",
    "eh = c_df.to_numpy()\n",
    "print(eh)\n",
    "eh[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chennai_config\n",
    "config = chennai_config.config\n",
    "def _fill_no_data_spots(data_list, label_list):\n",
    "    all_pkeys = set([1, 2, 4, 65, 70, 75]) # config[\"all_pkeys\"]\n",
    "    res = pd.DataFrame(all_pkeys, columns=[\"pkey\"]).set_index(\"pkey\", drop=False).sort_index()\n",
    "    for i, (data, labels) in enumerate(zip(data_list, label_list)):\n",
    "        add_df = pd.DataFrame(data.T, columns=[\"pkey\", i]).set_index(\"pkey\")\n",
    "        res = res.join(add_df).fillna(0)\n",
    "        # res.loc[add_df.index][i] = add_df.loc[a_df.index][i]\n",
    "        print(add_df.loc[a_df.index][i])\n",
    "        \n",
    "    flood = pd.Index([1, 2, 4])\n",
    "    no_flood = pd.Index([65, 70, 75])\n",
    "    # add in the label data as the last row\n",
    "    res.loc[flood, \"label\"] = 1\n",
    "    res.loc[no_flood, \"label\"] = -1\n",
    "    ahhh = res.to_numpy().T\n",
    "    # TODO there's probably some issue with floating point equality on the pkeys here... \n",
    "    return ahhh\n",
    "\n",
    "_fill_no_data_spots([a, a], [labs, labs] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_python",
   "language": "python",
   "name": "_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
